{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sg\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization\n",
    "    of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly,\n",
    "        so that ref_fls[5] is the reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)[\"sig\"]\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "# in your prediction file\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a saved sklearn regression model or train new model\n",
    "    Args:\n",
    "    Returns:\n",
    "        return the saved model\n",
    "    \"\"\"\n",
    "    path_n = model_path\n",
    "    if os.path.isfile(path_n):\n",
    "        fil = open(path_n, \"rb\")\n",
    "        prediction_model = pickle.load(fil)\n",
    "        print(\"model found\")\n",
    "    else:\n",
    "        prediction_model = model_train()\n",
    "        print(\"new model is created\")\n",
    "    return prediction_model\n",
    "\n",
    "\n",
    "def Load_labels(data_fl, ts):\n",
    "    \"\"\"\n",
    "    Load the reference PBM for a file and perform interpolation to convert it\n",
    "    to the same timestamp of the measurments\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "        ts: time stamp of the sensors measurements for the same file\n",
    "    Returns:\n",
    "        numpy arrays for ref BPM with the same length of sensor measurements\n",
    "    \"\"\"\n",
    "    # as mentioed in the dataset read_me the ref value is obtained using a\n",
    "    # window of 8s and 6s overlap\n",
    "    # so we can assume that the sample freq of the ref data is 2s\n",
    "    fs_labels = 2\n",
    "    data = sp.io.loadmat(data_fl)[\"BPM0\"]\n",
    "    ts_labels = np.arange(0, len(data) / fs_labels, 1 / fs_labels)\n",
    "    labels_interp = np.interp(ts, ts_labels, data[:, 0])\n",
    "    return labels_interp\n",
    "\n",
    "\n",
    "def butter_bandpass(data, rang, fs):\n",
    "    \"\"\"\n",
    "    bandpass the data withn certain frequency range for all sensors\n",
    "    Args:\n",
    "        data: a list of sensors measurements\n",
    "        range: tuple contain the min, max frequency\n",
    "        fs: sample frequence for the signal\n",
    "    Returns:\n",
    "        return a list arrays for all the filtered signals in the time domain\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    zeros, poles = sg.butter(5, rang, btype=\"bandpass\", fs=fs)\n",
    "    # filter each sensor measurement\n",
    "    for key in data:\n",
    "        results.append(sg.filtfilt(zeros, poles, key))\n",
    "    return results\n",
    "\n",
    "\n",
    "def fft_sensors(data, fs):\n",
    "    \"\"\"\n",
    "    fast fourier transform for the each sensor data\n",
    "    Args:\n",
    "        data: a list of sensors measurements\n",
    "        fs: sample frequence for the signal\n",
    "    Returns:\n",
    "        return a list of arrays for all the signals in the frequency domaoin\n",
    "        data is on this format data - > sensor1_freq, sensor1_powers\n",
    "                                    - > sensor2_freq, sensor2_powers\n",
    "                                    - > sensorn_freq, sensorn_powers\n",
    "    \"\"\"\n",
    "    ff_data = []\n",
    "    # FFT each sensor measurement\n",
    "    for key in data:\n",
    "        ff_freq = np.fft.rfftfreq(len(key), 1.0 / fs)\n",
    "        ff_power = np.fft.rfft(key)\n",
    "        ff_data.append(np.array([ff_freq, ff_power]))\n",
    "    return ff_data\n",
    "\n",
    "\n",
    "def fft_inv(fftdata):\n",
    "    \"\"\"\n",
    "    inverse fast fourier transform for each sensor data\n",
    "    Args:\n",
    "        data: a list of arrays for each sensor frequencies and powers\n",
    "        fs: sample frequence for the signal\n",
    "    Returns: list of arrays for each sensor signal in the time domain\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    # IFFT for each sensor data\n",
    "    for key in fftdata:\n",
    "        # Each sensor has freqs, power arrays so we chose the power here\n",
    "        data[key] = np.fft.irfft(fftdata[key][1])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Deprecated -> was used for visualization in the begining only\n",
    "def vizualize_data(data_dict_fs, wind_title, xlim, vtype=\"freq\"):\n",
    "    len_dict = len(data_dict_fs)\n",
    "    plt.clf\n",
    "    fig, ax = plt.subplots(len_dict, 1, figsize=(10, 6 * len_dict))\n",
    "    fig.canvas.set_window_title(wind_title)\n",
    "    ax = ax.flatten()\n",
    "    for key, axes in zip(data_dict_fs, ax):\n",
    "        if type == \"freq\":\n",
    "            power = np.abs(key[0])\n",
    "            freqs = np.abs(key[1])\n",
    "            axes.plot(freqs, power, label=key)\n",
    "            axes.set_xlabel(key)\n",
    "            axes.set_xlim(xlim)\n",
    "            axes.legend()\n",
    "        else:\n",
    "            axes.plot(key)\n",
    "            # axes.set_xlabel(key)\n",
    "            axes.set_xlim(xlim)\n",
    "            axes.legend()\n",
    "\n",
    "        # axes.set_xlabel('Time (sec)')\n",
    "\n",
    "\n",
    "def load_all_dataset(data_fls, ref_fls, fs, window_size=6, window_overlab=4):\n",
    "\n",
    "    \"\"\"\n",
    "    Load data, labels from all files, windowing on each 6 seconds data with\n",
    "    4 sec overlab each sample data is a window of 6 seconds and shifted 2 s\n",
    "    from the previoues one\n",
    "    Args:\n",
    "        data_fls: all the files for the dataset sensor measurements\n",
    "        ref_fls: all the files for the dataset sensor labels\n",
    "        fs: sample frequency\n",
    "    Returns: list of array for all samples in the dataset, each sample data\n",
    "     has all the sensors measurements in this sample in the time domain\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    window_len = 6 * fs\n",
    "    window_shift_len = (window_size - window_overlab) * fs\n",
    "\n",
    "    dataset = {\"ppg\": [], \"accx\": [], \"accy\": [], \"accz\": [], \"labels\": []}\n",
    "    # Read All Files\n",
    "    for data_file, ref_file in zip(data_fls, ref_fls):\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_file)\n",
    "        dataset[\"ppg\"].extend(ppg)\n",
    "        dataset[\"accx\"].extend(accx)\n",
    "        dataset[\"accy\"].extend(accy)\n",
    "        dataset[\"accz\"].extend(accz)\n",
    "        ts = np.arange(0, len(ppg) / fs, 1 / fs)\n",
    "        labels = Load_labels(ref_file, ts)\n",
    "        labels /= 60.0\n",
    "        dataset[\"labels\"].extend(labels)\n",
    "    # dataframe have data and labels for all the data and labels\n",
    "    # all files are appended\n",
    "    df = pd.DataFrame(dataset)\n",
    "    # Convert dataframe to samples each one is shifted 2s\n",
    "    number_samples = int(len(df) / window_shift_len)\n",
    "    for window_ind in range(number_samples):\n",
    "        sample_start = int(window_ind * window_shift_len)\n",
    "        sample_end = int(sample_start + window_len)\n",
    "        ppg = df[\"ppg\"][sample_start:sample_end].values\n",
    "        accx = df[\"accx\"][sample_start:sample_end].values\n",
    "        accy = df[\"accy\"][sample_start:sample_end].values\n",
    "        accz = df[\"accz\"][sample_start:sample_end].values\n",
    "        labels = df[\"labels\"][sample_start:sample_end].values\n",
    "        X.append(np.array([ppg, accx, accy, accz]))\n",
    "        Y.append(np.array([labels]))\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def clean_datset(samples, labels, fs):\n",
    "    \"\"\"\n",
    "    Clean each seample data by performing bandpass filter on range of 0.7-4\n",
    "    Hz and add FFT for each sample in the dataset\n",
    "    Args:\n",
    "        samples: list of array for each sample. each sample data has several\n",
    "         sensor measurements\n",
    "        labels: list of arrays for each sample labels\n",
    "    Returns: tuble of dataset time and frequency data for all sensors, labels\n",
    "     for each sample\n",
    "    \"\"\"\n",
    "    clean_x = []\n",
    "    clean_y = []\n",
    "    for sample, label in zip(samples, labels):\n",
    "        bandpass_filtered = butter_bandpass(sample, (0.7, 4), fs)\n",
    "        fft_sample = fft_sensors(bandpass_filtered, fs)\n",
    "        clean_x.append((bandpass_filtered, fft_sample))\n",
    "        clean_y.append([np.mean(label)])\n",
    "    return (clean_x, clean_y)\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability.\n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and\n",
    "         corresponding reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse\n",
    "         rate error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "\n",
    "def remove_powerful_acc_freqs_from_ppg(\n",
    "    peaks_freq_ppg, peaks_freqs_ppg_vals, peaks_freq_acc\n",
    "):\n",
    "    \"\"\"\n",
    "    in any sample, filter the ppg peak frequencies that exist in the\n",
    "     acceleration peak frequencies .\n",
    "    If an acceleration frequency exists within +-0.25Hz range for any of\n",
    "     the ppg peak freqs, this freq is removed.\n",
    "    Args:\n",
    "        peaks_freq_ppg: a list of the powerful freqs in the ppg freq signal\n",
    "        peaks_freqs_ppg_vals: power of each ppg peak frequency\n",
    "        peaks_freq_acc:a list of the powerful freqs in the accel freq signal\n",
    "    Returns:return a tuple of the most powerful ppg freq and its power after\n",
    "     removing near acc freqs\n",
    "    \"\"\"\n",
    "    # if there is only one peak in this sample, take this sample\n",
    "    if len(peaks_freq_ppg) > 1:\n",
    "\n",
    "        # sort ppg freqs according to their power\n",
    "        sorted_ppg_freqs = peaks_freq_ppg[np.argsort(peaks_freqs_ppg_vals)]\n",
    "        sorted_ppg_powers = peaks_freqs_ppg_vals[np.argsort(\n",
    "                                                peaks_freqs_ppg_vals)]\n",
    "        ppg_freqs_acc_removed = []\n",
    "        ppg_freqs_acc_removed_val = []\n",
    "        # for each ppg freq search for powerful freqs near the range\n",
    "        for freq, power in zip(sorted_ppg_freqs, sorted_ppg_powers):\n",
    "            flag = 0\n",
    "            for acc_freq in peaks_freq_acc:\n",
    "                if (freq + 0.15 <= acc_freq) & (acc_freq <= freq + 0.15):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 0:\n",
    "                ppg_freqs_acc_removed.append(freq)\n",
    "                ppg_freqs_acc_removed_val.append(power)\n",
    "\n",
    "        ppg_1st_power_freq_acc_removed = ppg_freqs_acc_removed[-1]\n",
    "        ppg_1st_power_freq_acc_removed_val = ppg_freqs_acc_removed_val[-1]\n",
    "    else:\n",
    "        ppg_1st_power_freq_acc_removed = peaks_freq_ppg[-1]\n",
    "        ppg_1st_power_freq_acc_removed_val = peaks_freqs_ppg_vals[-1]\n",
    "    return (ppg_1st_power_freq_acc_removed, ppg_1st_power_freq_acc_removed_val)\n",
    "\n",
    "\n",
    "def featurize_samples(samples, fs):\n",
    "    \"\"\"\n",
    "    samples->(time_data,freq_data)->(ppg,accx,accy,accz)\n",
    "                                    ->fppg,faccx,faccy,faccz->freqs,power\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    time and freq featurization for each sample sensors data\n",
    "    Args:\n",
    "        samples: a list of the dataset samples\n",
    "        fs: sample frequency\n",
    "    Returns: return a list of arrays for each sample features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        # extract each sample to each sensor time & freq data\n",
    "        ppg_t = sample[0][0]\n",
    "        freqs = np.abs(sample[1][0][0])\n",
    "        ppg_f = np.abs(sample[1][0][1])\n",
    "        accx_f = np.abs(sample[1][1][1])\n",
    "        accy_f = np.abs(sample[1][2][1])\n",
    "        accz_f = np.abs(sample[1][3][1])\n",
    "        #  average freq power for all accel axes\n",
    "        acc_all_f = (accx_f + accy_f + accz_f) / 3.0\n",
    "\n",
    "        # Time features\n",
    "        peaks, _ = sg.find_peaks(\n",
    "            ppg_t, height=1.5, distance=30\n",
    "        )\n",
    "        # minimum 0.25s distance(4Hz)\n",
    "        # periods features\n",
    "        peaks_dif = np.diff(peaks) / 60.0\n",
    "        min_peak = np.min(peaks_dif)\n",
    "        max_peak = np.max(peaks_dif)\n",
    "        mean_peak = np.mean(peaks_dif)\n",
    "        std_peak = np.std(peaks_dif)\n",
    "\n",
    "        # peaks for ppg freqs & their power\n",
    "        # minimum 1/3Hz distance\n",
    "        peaks_freq_ppg, _ = sg.find_peaks(ppg_f, distance=3)\n",
    "        peaks_freqs_ppg_vals = ppg_f[peaks_freq_ppg]\n",
    "        peaks_freq_ppg = freqs[peaks_freq_ppg]\n",
    "        # take the peaks in the allowed freq only\n",
    "        peaks_freqs_ppg_vals = peaks_freqs_ppg_vals[\n",
    "            (peaks_freq_ppg < 4.0) & (peaks_freq_ppg > 0.67)\n",
    "        ]\n",
    "        peaks_freq_ppg = peaks_freq_ppg[\n",
    "            (peaks_freq_ppg < 4.0) & (peaks_freq_ppg > 0.67)\n",
    "        ]\n",
    "        # peaks for accel freqs & their power\n",
    "        peaks_freq_acc, _ = sg.find_peaks(\n",
    "            acc_all_f, distance=3\n",
    "        )  # minimum 1/3Hz distance\n",
    "        peaks_freqs_acc_vals = acc_all_f[peaks_freq_acc]\n",
    "        peaks_freq_acc = freqs[peaks_freq_acc]\n",
    "        peaks_freqs_acc_vals = peaks_freqs_acc_vals[\n",
    "            (peaks_freq_acc < 4.0) & (peaks_freq_acc > 0.67)\n",
    "        ]\n",
    "        peaks_freq_acc = peaks_freq_acc[\n",
    "            (peaks_freq_acc < 4.0) & (peaks_freq_acc > 0.67)\n",
    "        ]\n",
    "        # freq features for ppg\n",
    "        ppg_freq_mean = np.mean(peaks_freq_ppg)\n",
    "        ppg_freq_std = np.std(peaks_freq_ppg)\n",
    "        ppg_1st_power_freq = peaks_freq_ppg[np.argsort(\n",
    "                                    peaks_freqs_ppg_vals)][-1]\n",
    "        ppg_1st_power_freq_val = peaks_freqs_ppg_vals[np.argsort(\n",
    "                                    peaks_freqs_ppg_vals)][-1]\n",
    "        (ppg_1st_freq_acc_fil, ppg_1st_freq_acc_fil_val,) = \\\n",
    "            remove_powerful_acc_freqs_from_ppg(\n",
    "                        peaks_freq_ppg, peaks_freqs_ppg_vals, peaks_freq_acc\n",
    "                                    )\n",
    "\n",
    "        # freq features for acc\n",
    "        acc_all_freq_mean = np.mean(peaks_freq_acc)\n",
    "        acc_all_freq_std = np.std(peaks_freq_acc)\n",
    "        acc_all_max_strong_freq = peaks_freq_acc[np.argsort(\n",
    "                                            peaks_freqs_acc_vals)][-1]\n",
    "        acc_all_max_strong_freq_power = peaks_freqs_acc_vals[\n",
    "            np.argsort(peaks_freqs_acc_vals)\n",
    "        ][-1]\n",
    "\n",
    "        time_features = [min_peak, max_peak, mean_peak, std_peak]\n",
    "        freq_features = [\n",
    "            ppg_freq_mean,\n",
    "            ppg_freq_std,\n",
    "            ppg_1st_power_freq,\n",
    "            ppg_1st_power_freq_val,\n",
    "            ppg_1st_freq_acc_fil,\n",
    "            ppg_1st_freq_acc_fil_val,\n",
    "            acc_all_freq_mean,\n",
    "            acc_all_freq_std,\n",
    "            acc_all_max_strong_freq,\n",
    "            acc_all_max_strong_freq_power,\n",
    "        ]\n",
    "        total_features = time_features + freq_features\n",
    "        features.append(np.array(total_features))\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def model_train(estimators=650, depth=14, file_path=\"model_final\"):\n",
    "    \"\"\"\n",
    "    train a random forest regressor on the dataset and save it in the\n",
    "     provided path\n",
    "    Args:\n",
    "        estimators: number of trees in the model\n",
    "        depth: single value for the depth of each tree of the model\n",
    "        file_path: a path string to which model will be saved\n",
    "    Returns: return the trained regression model\n",
    "    \"\"\"\n",
    "    # Reading ref and sensors data, create timestamp for both\n",
    "    fs = 125\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    samples, labels = load_all_dataset(data_fls, ref_fls, fs)\n",
    "    clean_x, clean_y = clean_datset(samples, labels, fs)\n",
    "    \"\"\"\n",
    "    samples->(time_data,freq_data)->(ppg,accx,accy,accz)\n",
    "                                    ->fppg,faccx,faccy,faccz->freqs,power\n",
    "    \"\"\"\n",
    "    dataset_feats = featurize_samples(clean_x, fs)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        dataset_feats, clean_y, random_state=42, test_size=0.2\n",
    "    )\n",
    "    clf = RandomForestRegressor(\n",
    "        n_estimators=estimators, max_depth=depth, random_state=42\n",
    "    )\n",
    "    clf.fit(train_x, np.ravel(train_y))\n",
    "    y_pred = clf.predict(test_x)\n",
    "    mae_val = mean_absolute_error(\n",
    "        y_pred * 60.0, np.array(test_y) * 60.0\n",
    "    )  # *60 toconvert from Hz t BPM\n",
    "    mse_val = mean_squared_error(\n",
    "        y_pred * 60, np.array(test_y) * 60\n",
    "    )  # *60 to convert from Hz t BPM\n",
    "    # res={\"label\":np.ravel(test_y)*60,\"prediction\":y_pred*60}\n",
    "    print(\"Model Results : \\n\")\n",
    "    print(\"mean absolute error\", mae_val)\n",
    "    print(\"mean square error\", mse_val)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "        print(\"model saved in the following dir: %s\" % file_path)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an\n",
    "     aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        # errors=list(errors)\n",
    "        # confidence=list(confidence)\n",
    "        # print(errors.shape,confidence.shape)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    # print(len(errs),len(confs))\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    \"\"\"\n",
    "    load regresssion model, perform prediction over a file  samples\n",
    "    return mean error and confidence for each sample\n",
    "    Args:\n",
    "        data_fl: file name for sensor measurements\n",
    "        ref_fl: file name that has true PBM value\n",
    "        file_path: a path string to which model will be saved\n",
    "    Returns: tuple of two arrays for mean error confidence\n",
    "    \"\"\"\n",
    "    model_path = \"model_final\"\n",
    "    fs = 125\n",
    "    samples, labels = load_all_dataset([data_fl], [ref_fl], fs)\n",
    "    clean_x, clean_y = clean_datset(samples, labels, fs)\n",
    "    file_samples_x = featurize_samples(clean_x, fs)\n",
    "    file_samples_y = np.array(clean_y)\n",
    "    # ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    reg_model = load_model(model_path)\n",
    "    samples_pred = reg_model.predict(file_samples_x)\n",
    "\n",
    "    mae_val = np.abs(\n",
    "        (samples_pred.reshape(-1, 1)) * 60.0 - np.array(file_samples_y) * 60.0\n",
    "    )  # *60 toconvert from Hz t BPM\n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    confidence = []\n",
    "    mae_val = np.ravel(mae_val)\n",
    "    for ind in range(len(clean_y)):\n",
    "        pred_freq = samples_pred[ind]\n",
    "        fft_freqs = np.abs(clean_x[ind][1][0][0])\n",
    "        fft_f_power = np.abs(clean_x[ind][1][0][1])\n",
    "        freq_wind = \\\n",
    "            (fft_freqs > pred_freq - 0.5) & (fft_freqs < pred_freq + 0.5)\n",
    "        conf = np.sum(fft_f_power[freq_wind]) / np.sum(fft_f_power)\n",
    "        confidence.append(conf)\n",
    "    # print(mae_val.shape,np.array(confidence).shape)\n",
    "    # Return per-estimate mean absolute error and confidence as\n",
    "    # a 2-tuple of numpy arrays.\n",
    "    return mae_val, np.array(confidence)\n",
    "\n",
    "\n",
    "def get_predictions(data_fl, ref_fl, model_path=\"model_final\"):\n",
    "    \"\"\"\n",
    "    load regresssion model, perform prediction over a file  samples\n",
    "    return prediction and confidence for each sample\n",
    "    Args:\n",
    "        data_fl: file name for sensor measurements\n",
    "        model_path: a path string to which model will be saved\n",
    "    Returns: a data for predictions and confidence of each 2s measurement\n",
    "    \"\"\"\n",
    "    fs = 125\n",
    "    samples, labels = load_all_dataset([data_fl], [ref_fl], fs)\n",
    "    clean_x, clean_y = clean_datset(samples, labels, fs)\n",
    "    file_samples_x = featurize_samples(clean_x, fs)\n",
    "    file_samples_y = np.array(clean_y)\n",
    "    # ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    reg_model = load_model(model_path)\n",
    "    samples_pred = reg_model.predict(file_samples_x)\n",
    "    confidence = []\n",
    "    for ind in range(len(clean_y)):\n",
    "        pred_freq = samples_pred[ind]\n",
    "        fft_freqs = np.abs(clean_x[ind][1][0][0])\n",
    "        fft_f_power = np.abs(clean_x[ind][1][0][1])\n",
    "        freq_wind = \\\n",
    "            (fft_freqs > pred_freq - 0.5) & (fft_freqs < pred_freq + 0.5)\n",
    "        conf = np.sum(fft_f_power[freq_wind]) / np.sum(fft_f_power)\n",
    "        confidence.append(conf)\n",
    "    # print(len(file_samples_y),len(samples_pred),len(confidence))\n",
    "    results = {\n",
    "        \"true PPM\": np.ravel(file_samples_y) * 60.0,\n",
    "        \"predicted_PPM\": samples_pred * 60.0,\n",
    "        \"confidence\": confidence,\n",
    "    }\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Results : \n",
      "\n",
      "mean absolute error 8.600705195109164\n",
      "mean square error 166.6567147439998\n",
      "model saved in the following dir: model_final\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=14,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=650,\n",
       "                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train(estimators=650, depth=14, file_path=\"model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n",
      "model found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.82323605387397"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "##### Running the code\n",
    " 1. download all requred libraries are avaiable and all dependancies with Python 3.7\n",
    " 2. to run test if everything is working, you can use ```RunPulseRateAlgorithm``` function. To use this function you should provide 1 file that have PPG , 3-axis Accelerometer sensors measurements and 1 more file that have the true BPM values. All files should be the same format to Troika Dataset.\n",
    " 3. to run the training, testing process at the same at once, you can use ```Evaluate``` function which will do everything and return the mean average error of the algorithm. Troika dataset should be provided in the same directory of the notebook in a folder called 'datasets'\n",
    " 4. to run the project for getting BPM values for certain file you can run the following code\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true PPM</th>\n",
       "      <th>predicted_PPM</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>153.058110</td>\n",
       "      <td>0.458667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.790161</td>\n",
       "      <td>0.438753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>163.973422</td>\n",
       "      <td>157.487565</td>\n",
       "      <td>0.536739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>154.490148</td>\n",
       "      <td>153.304711</td>\n",
       "      <td>0.513044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.225355</td>\n",
       "      <td>88.902268</td>\n",
       "      <td>0.376885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.264625</td>\n",
       "      <td>0.374124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.766573</td>\n",
       "      <td>0.402636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.270538</td>\n",
       "      <td>0.468361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.119899</td>\n",
       "      <td>0.422420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>154.220779</td>\n",
       "      <td>154.837754</td>\n",
       "      <td>0.496803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       true PPM  predicted_PPM  confidence\n",
       "147  154.220779     153.058110    0.458667\n",
       "72   154.220779     154.790161    0.438753\n",
       "31   163.973422     157.487565    0.536739\n",
       "36   154.490148     153.304711    0.513044\n",
       "1     74.225355      88.902268    0.376885\n",
       "144  154.220779     154.264625    0.374124\n",
       "84   154.220779     154.766573    0.402636\n",
       "145  154.220779     154.270538    0.468361\n",
       "111  154.220779     154.119899    0.422420\n",
       "130  154.220779     154.837754    0.496803"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fs,ref_fs=LoadTroikaDataset()\n",
    "df=get_predictions(data_fs[0],ref_fs[0])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "\n",
    "#### Description\n",
    "- The used dataset is a modiffied version of the Troika dataset recorded from 12 male subjects with yellow skin and ages ranging from 18 to 35.\n",
    "- Dataset provide measurements from PPG ,3-axis Acceleromenter at sample freuquency = 125 Hz\n",
    "- For each subject, the PPG signal was recorded from wrist using a pulse oximeter with green LED (wavelength: 515nm).\n",
    "- The acceleration signal was also recorded from wrist using a three-axis accelerometer. Both the pulse oximeter and the accelerometer were embedded in a wristband, which was comfortably worn. \n",
    "\n",
    "- During data recording subjects walked or ran on a treadmill with the following speeds in order: the speed of 1-2 km/hour for 0.5 minute, the speed of 6-8 km/hour for 1 minute, the speed of 12-15 km/hour for 1 minute, the speed of 6-8 km/hour for 1 minutes, the speed of 12-15 km/hour for 1 minute, and the speed of 1-2 km/hour for 0.5 minute. The subjects were asked to purposely use the hand with the wristband to pull clothes, wipe sweat on forehead in addition to freely swing\n",
    "\n",
    "#### Limitations\n",
    "- predictions can have bias towards the provided age range, males, yellow skin as a result of the unbalanced distrubution of the dataset\n",
    "- predictions can have small bias to the used pattern when collecting the data\n",
    "- predictions can have bias towards the used sensors when using different sensors\n",
    "- predictions may have high mean average error when predicting PPM at rest.\n",
    "#### Optimum dataset\n",
    "- demographic distribution should represent the real world in (age, gender,skin color)\n",
    "- size of the dataset should be increased  to decrease any chacn of overfitting\n",
    "- more daily life activitites should be monitored too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Description\n",
    "#### Preprocessing\n",
    "- sensors time series are filtered with bandpass filter within 40-240BPM\n",
    "- windowing algorithm is performed to split each data series to 6s samples and each sample is overlapped 4s with the ealrier one to achieve 2Hz output\n",
    "- FFT(fast fourier transform) is performed to convert samples to frequency domain\n",
    "- each sample time and frequency measurements are packed together for featurization process\n",
    "#### Featurization\n",
    "\n",
    " For each sample the following features was taken:  \n",
    " - minimum time between 2 sequential peaks in ppg singal\n",
    " - maximum time between 2 sequential peaks in ppg singal\n",
    " - the mean time between 2 sequential peaks in ppg singal \n",
    " - standard deviation between 2 sequential peaks in ppg singal \n",
    " - mean frequency in the ppg signal\n",
    " - standard deviation for frequency in ppg signal\n",
    " - most powerful freq in ppg singal and it's value\n",
    " - most powerful freq in acceleration singal and it's value\n",
    " - acceleration frequencies mean and standard deviation\n",
    " - most powerful freq in ppg singal that doesn't exist in the acceleration freqs and it's value \n",
    "#### Regression model\n",
    "Algorithm takes all the features of each sample and feed this data to a regression Random Forest model which is trained using 650 tree and a depth of 12 for each tree.\n",
    " \n",
    "#### Physiology \n",
    "This algorithm mainly input is from PPG sensors which uses emits light through skin and recives this signal again and depending on the absorbtion rate of the recevied light we can tell weather this measuremet is for blood that is going to the body or towards heart because the absorbtion rate is different for blood with high and low red cells percentage. As a result we can know when conctraction and retraction of the heart happen.\n",
    "#### Output\n",
    "\n",
    "Algorithm privodes heart rate measurements in PPM and the fonfidence of each measurement depending on the measured power around this frequency.\n",
    "\n",
    "#### Caveats on algorithm outputs\n",
    "\n",
    "- algorithm isn't still general for any dataset format and files should have same format as Troika dataset\n",
    "- the function that gives prediction still requires reference file because it uses several preprocessing, cleaning, filtering functions that was used for training although the ref file don't affect the prediction process by any way.\n",
    "- no specific failure cases was noticed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Performance\n",
    "#### Performance \n",
    "\n",
    "- performance was computed using mean absolute error MAE metric over both 90th precentile and 10th percntile \n",
    "- dataset was split and shuffled using train_test split sklearn functions and the error was measured over the testset \n",
    "- overall MAE= 6.8\n",
    "- Confidence of the model can also be used as indication to the error in the measurements. When the confidence is low this means that this PPM have ony small power in the obtained data or the measurements have other high power frequencies created by movement noise or any other distortion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
